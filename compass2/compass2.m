%% compass2 ece656
% cnn and bpnn with LM networks
clear all
clc
% The CNN path contains something that breaks the builtin feedforward nets,
% reset the path to launch default
path(pathdef)
%%
total_num_training = 60000;
total_num_testing = 10000;
total_num = total_num_training  + total_num_testing;
% Read in all training and testing images and labels
[trainimgs,trainlabels,testimgs,testlabels] = readMNIST(total_num, 'MNIST');

% combine the train and test sets to form our own train, validation and
% test sets
allimgs = [trainimgs, testimgs];
alllabels = [trainlabels; testlabels]';
alllabels10 = num2bin10(alllabels);

%% Uniformly sample the training set to extract 15% of the samples.  This choosed approx equal number from each class
[trainsamples trainsamplesidxs] = datasample(allimgs,size(allimgs,2)*.20, 'Replace', false);
trainsampleslabels = alllabels(trainsamplesidxs);
trainsampleslabels10 = num2bin10(trainsampleslabels);

% find the non sampled images and store them
othersamplesidxs = setdiff(1:size(allimgs,2), trainsamplesidxs);
othersamples = allimgs(othersamplesidxs);
othersampleslabel = alllabels(othersamplesidxs);

% generate testing and validation sets
numothers = numel(othersamples);
[testsamples testsamplesidxs] = datasample(othersamples,numothers*.5, 'Replace', false);
testsampleslabels = othersampleslabel(testsamplesidxs);

validsamplesidxs = setdiff(1:numothers, testsamplesidxs);
validsamples = othersamples(validsamplesidxs);
validsampleslabel = othersampleslabel(validsamplesidxs);


% sanity check for uniform sampling
% trainsampleslabels3 = trainsampleslabels;
% trainsampleslabels3(trainsampleslabels~=3)=0;
% trainsampleslabels3(trainsampleslabels==3)=1;
% sum(trainsampleslabels3)

% FIXME - may want to do data normalization
%% Vectorize all samples
trainsamples_vec = zeros(28*28, numel(trainsamples));
allimgs_vec = zeros(28*28, numel(allimgs));
% imshow(reshape(trainsamples_vec(:,1), 28, 28))
for i = 1:numel(trainsamples)
    trainsamples_vec(:, i) = reshape(trainsamples{i}, 28*28, 1);
end
for i = 1:numel(allimgs)
    allimgs_vec(:, i) = reshape(allimgs{i}, 28*28, 1);
end

%%
% Solve a Pattern Recognition Problem with a Neural Network
% Script generated by Neural Pattern Recognition app
% Created 25-Mar-2018 20:08:07
%
% This script assumes these variables are defined:
%
%   allimgs_vec - input data.
%   alllabels10 - target data.


trainx = allimgs_vec(:, 1:end-10000);
traint = alllabels10(:, 1:end-10000);
testx = allimgs_vec(:, end-10000:end);
testt = alllabels10(:, end-10000:end);

x = trainx;
t = traint;

% Choose a Training Function
% For a list of all training functions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. Suitable in low memory situations.
trainFcn = 'trainscg';  % Scaled conjugate gradient backpropagation.

% Create a Pattern Recognition Network
hiddenLayerSize = 84;
net = patternnet(hiddenLayerSize);

% Choose Input and Output Pre/Post-Processing Functions
% For a list of all processing functions type: help nnprocess
net.input.processFcns = {'removeconstantrows','mapminmax'};
net.output.processFcns = {'removeconstantrows','mapminmax'};

% Setup Division of Data for Training, Validation, Testing
% For a list of all data division functions type: help nndivide
net.divideFcn = 'dividerand';  % Divide data randomly
net.divideMode = 'sample';  % Divide up every sample
net.divideParam.trainRatio = 50/100;
net.divideParam.valRatio = 25/100;
net.divideParam.testRatio = 25/100;

% Choose a Performance Function
% For a list of all performance functions type: help nnperformance
net.performFcn = 'crossentropy';  % Cross-Entropy

% Choose Plot Functions
% For a list of all plot functions type: help nnplot
net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
    'plotconfusion', 'plotroc'};

trainnet = 1
if (trainnet)
    % Train the Network
    [net,tr] = train(net,x,t);
    save('trained_bpnn_new.mat', 'net', 'tr')
else
    load('trained_bpnn.mat')
end
%%
%% Test the Network
y = net(testx);
e = gsubtract(testt,y);
performance = perform(net,testt,y)
tind = vec2ind(y);
yind = vec2ind(y);
percentErrors = sum(tind ~= yind)/numel(tind);

% Recalculate Training, Validation and Test Performance
% trainTargets = t .* tr.trainMask{1};
% valTargets = t .* tr.valMask{1};
% testTargets = t .* tr.testMask{1};
% trainPerformance = perform(net,trainTargets,y)
% valPerformance = perform(net,valTargets,y)
% testPerformance = perform(net,testTargets,y)

% View the Network
% view(net)

% Plots
% Uncomment these lines to enable various plots.
figure(1), plotperform(tr)
figure(2), plottrainstate(tr)
figure(3), ploterrhist(e)
figure(4), plotconfusion(testt,y)
figure(5), plotroc(testt,y)

% Deployment
% Change the (false) values to (true) to enable the following code blocks.
% See the help for each generation function for more information.
if (false)
    % Generate MATLAB function for neural network for application
    % deployment in MATLAB scripts or with MATLAB Compiler and Builder
    % tools, or simply to examine the calculations your trained neural
    % network performs.
    genFunction(net,'myNeuralNetworkFunction');
    y = myNeuralNetworkFunction(x);
end
if (false)
    % Generate a matrix-only MATLAB function for neural network code
    % generation with MATLAB Coder tools.
    genFunction(net,'myNeuralNetworkFunction','MatrixOnly','yes');
    y = myNeuralNetworkFunction(x);
end
if (false)
    % Generate a Simulink diagram for simulation or deployment with.
    % Simulink Coder tools.
    gensim(net);
end
